
@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}, second edition: {An} {Introduction}},
	isbn = {978-0-262-35270-3},
	shorttitle = {Reinforcement {Learning}, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = nov,
	year = {2018},
	keywords = {Computers / Artificial Intelligence / General},
}

@article{barron_bellman_1989,
	title = {The {Bellman} equation for minimizing the maximum cost},
	volume = {13},
	issn = {0362546X},
	doi = {10.1016/0362-546X(89)90096-5},
	number = {9},
	journal = {Nonlinear Analysis: Theory, Methods \& Applications},
	author = {Barron, E.N. and Ishii, H.},
	month = sep,
	year = {1989},
	file = {Barron and Ishii - 1989 - The Bellman equation for minimizing the maximum co.pdf:/home/daveg/Zotero/storage/VYN8LUJG/Barron and Ishii - 1989 - The Bellman equation for minimizing the maximum co.pdf:application/pdf},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	number = {3},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {asynchronous dynamic programming, Q-learning, reinforcement learning, temporal differences},
	pages = {279--292},
	file = {Full Text PDF:/home/daveg/Zotero/storage/VF2BNQLA/Watkins and Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@article{schaul_studies_2011,
	title = {Studies in {Continuous} {Black}-box {Optimization}},
	author = {Schaul, Tom},
	year = {2011},
	file = {Schaul - Studies in Continuous Black-box Optimization.pdf:/home/daveg/Zotero/storage/3VFHUB8U/Schaul - Studies in Continuous Black-box Optimization.pdf:application/pdf},
}

@misc{noauthor_box2d_nodate,
	title = {{Box2D}: {Overview}},
	url = {https://box2d.org/documentation/index.html},
	file = {Box2D\: Overview:/home/daveg/Zotero/storage/FA5V42CV/index.html:text/html},
}

@article{elsken_neural_2019,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	year = {2019},
	file = {Elsken et al. - Neural Architecture Search A Survey.pdf:/home/daveg/Zotero/storage/Y96PIE8L/Elsken et al. - Neural Architecture Search A Survey.pdf:application/pdf},
}

@misc{oller_analyzing_2020,
	title = {Analyzing {Reinforcement} {Learning} {Benchmarks} with {Random} {Weight} {Guessing}},
	abstract = {We propose a novel method for analyzing and visualizing the complexity of standard reinforcement learning (RL) benchmarks based on score distributions. A large number of policy networks are generated by randomly guessing their parameters, and then evaluated on the benchmark task; the study of their aggregated results provide insights into the benchmark complexity. Our method guarantees objectivity of evaluation by sidestepping learning altogether: the policy network parameters are generated using Random Weight Guessing (RWG), making our method agnostic to (i) the classic RL setup, (ii) any learning algorithm, and (iii) hyperparameter tuning. We show that this approach isolates the environment complexity, highlights speciﬁc types of challenges, and provides a proper foundation for the statistical analysis of the task’s difﬁculty. We test our approach on a variety of classic control benchmarks from the OpenAI Gym, where we show that small untrained networks can provide a robust baseline for a variety of tasks. The networks generated often show good performance even without gradual learning, incidentally highlighting the triviality of a few popular benchmarks.},
	publisher = {arXiv},
	author = {Oller, Declan and Glasmachers, Tobias and Cuccu, Giuseppe},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Multiagent Systems},
	file = {Oller et al. - 2020 - Analyzing Reinforcement Learning Benchmarks with R.pdf:/home/daveg/Zotero/storage/TNN85XTV/Oller et al. - 2020 - Analyzing Reinforcement Learning Benchmarks with R.pdf:application/pdf},
}

@article{masanti_alternative_2022,
	title = {Alternative {Models} for {Direct} {Policy} {Search} in {Reinforcement} {Learning} {Control} {Problems}},
	author = {Masanti, Corina},
	year = {2022},
	file = {Masanti - Alternative Models for Direct Policy Search in Rei.pdf:/home/daveg/Zotero/storage/5W6MSAXC/Masanti - Alternative Models for Direct Policy Search in Rei.pdf:application/pdf},
}

@article{mellor_neural_2021,
	title = {Neural {Architecture} {Search} without {Training}},
	abstract = {The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be alleviated if we could partially predict a network’s trained accuracy from its initial state. In this work, we examine the overlap of activations between datapoints in untrained networks and motivate how this can give a measure which is usefully indicative of a network’s trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU, and verify its effectiveness on NAS-Bench-101, NASBench-201, NATS-Bench, and Network Design Spaces. Our approach can be readily combined with more expensive search methods; we examine a simple adaptation of regularised evolutionary search. Code for reproducing our experiments is available at https://github.com/ BayesWatch/nas-without-training.},
	author = {Mellor, Joseph and Turner, Jack and Storkey, Amos and Crowley, Elliot J},
	year = {2021},
	file = {Mellor et al. - Neural Architecture Search without Training.pdf:/home/daveg/Zotero/storage/BTHMWL49/Mellor et al. - Neural Architecture Search without Training.pdf:application/pdf},
}

@article{coggan_exploration_2004,
	title = {Exploration and {Exploitation} in {Reinforcement} {Learning}},
	author = {Coggan, Melanie},
	year = {2004},
	file = {Coggan - Exploration and Exploitation in Reinforcement Lear.pdf:/home/daveg/Zotero/storage/TVKWHEMY/Coggan - Exploration and Exploitation in Reinforcement Lear.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {978-0-262-33737-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
	keywords = {Computers / Artificial Intelligence / General, Computers / Computer Science},
}

@book{sutton_reinforcement_1998,
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-30384-2},
	shorttitle = {Reinforcement {Learning}},
	abstract = {Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = feb,
	year = {1998},
	keywords = {Computers / Artificial Intelligence / General},
}

@book{franklin_feedback_2014,
	address = {USA},
	edition = {7th},
	title = {Feedback {Control} of {Dynamic} {Systems}},
	isbn = {0-13-349659-7},
	abstract = {Feedback Control of Dynamic Systems covers the material that every engineer, and most scientists and prospective managers, needs to know about feedback controlincluding concepts like stability, tracking, and robustness. Each chapter presents the fundamentals along with comprehensive, worked-out examples, all within a real-world context and with historical background information. The authors also provide case studies with close integration of MATLAB throughout. Teaching and Learning Experience This program will provide a better teaching and learning experiencefor you and your students. It will provide: An Understandable Introduction to Digital Control: This text is devoted to supporting students equally in their need to grasp both traditional and more modern topics of digital control. Real-world Perspective: Comprehensive Case Studies and extensive integrated MATLAB/SIMULINK examples illustrate real-world problems and applications. Focus on Design: The authors focus on design as a theme early on and throughout the entire book, rather than focusing on analysis first and design much later.},
	publisher = {Prentice Hall Press},
	author = {Franklin, Gene F. and Powell, J. Da and Emami-Naeini, Abbas},
	year = {2014},
}

@article{goodrich_data_2013,
	title = {Data {Structures} and {Algorithms} in {Python}},
	author = {Goodrich, Michael T},
	year = {2013},
	file = {Goodrich - Data Structures and Algorithms in Python.pdf:/home/daveg/Zotero/storage/8Z6PD2VI/Goodrich - Data Structures and Algorithms in Python.pdf:application/pdf},
}

@misc{hansen_cma-espycma_2019,
	title = {{CMA}-{ES}/pycma on {Github}},
	author = {Hansen, Nikolaus and Akimoto, Youhei and Baudis, Petr},
	month = feb,
	year = {2019},
}

@phdthesis{cuccu_extending_2018,
	address = {Fribourg, Switzerland},
	type = {{PhD} {Thesis}},
	title = {Extending the {Applicability} of {Neuroevolution}},
	school = {Université de Fribourg},
	author = {Cuccu, Giuseppe},
	year = {2018},
}

@book{anderson_introduction_1995,
	title = {An introduction to neural networks},
	publisher = {MIT press},
	author = {Anderson, James A},
	year = {1995},
}

@misc{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	year = {2016},
	note = {\_eprint: 1606.01540},
}

@misc{cuccu_playing_2019,
	title = {Playing {Atari} with {Six} {Neurons}},
	author = {Cuccu, Giuseppe and Togelius, Julian and Cudre-Mauroux, Philippe},
	year = {2019},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI} {Gym} {Beta}},
	url = {https://openai.com/research/openai-gym-beta},
	abstract = {We’re releasing the public beta of OpenAI Gym, a toolkit for developing and comparing reinforcement learning (RL) algorithms. It consists of a growing suite of environments (from simulated robots to Atari games), and a site for comparing and reproducing results.},
	file = {Snapshot:/home/daveg/Zotero/storage/K8QYPZHA/openai-gym-beta.html:text/html},
}

@article{recht_tour_2019,
	title = {A {Tour} of {Reinforcement} {Learning}: {The} {View} from {Continuous} {Control}},
	volume = {2},
	doi = {10.1146/annurev-control-053018-023825},
	abstract = {This article surveys reinforcement learning from the perspective of optimization and control, with a focus on continuous control applications. It reviews the general formulation, terminology, and typical experimental implementations of reinforcement learning as well as competing solution paradigms. In order to compare the relative merits of various techniques, it presents a case study of the linear quadratic regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. It also describes how merging techniques from learning theory and control can provide nonasymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. The article concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.},
	number = {1},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Recht, Benjamin},
	year = {2019},
	pages = {253--279},
}

@misc{salimans_evolution_2017,
	title = {Evolution {Strategies} as a {Scalable} {Alternative} to {Reinforcement} {Learning}},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
	year = {2017},
}

@misc{zhu_ingredients_2020,
	title = {The {Ingredients} of {Real}-{World} {Robotic} {Reinforcement} {Learning}},
	author = {Zhu, Henry and Yu, Justin and Gupta, Abhishek and Shah, Dhruv and Hartikainen, Kristian and Singh, Avi and Kumar, Vikash and Levine, Sergey},
	year = {2020},
	note = {\_eprint: 2004.12570},
}

@article{schmidhuber_evaluating_2001,
	title = {Evaluating benchmark problems by random guessing},
	journal = {A Field Guide to Dynamical Recurrent Networks},
	author = {Schmidhuber, Jürgen and Hochreiter, Sepp and Bengio, Yoshua},
	year = {2001},
	note = {Publisher: Wiley-IEEE Press, Hoboken, NJ, USA},
	pages = {231--235},
}
