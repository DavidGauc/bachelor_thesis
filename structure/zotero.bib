
@article{barron_bellman_1989,
	title = {The {Bellman} equation for minimizing the maximum cost},
	volume = {13},
	issn = {0362546X},
	doi = {10.1016/0362-546X(89)90096-5},
	number = {9},
	journal = {Nonlinear Analysis: Theory, Methods \& Applications},
	author = {Barron, E.N. and Ishii, H.},
	month = sep,
	year = {1989},
	file = {Barron and Ishii - 1989 - The Bellman equation for minimizing the maximum co.pdf:/home/daveg/Zotero/storage/VYN8LUJG/Barron and Ishii - 1989 - The Bellman equation for minimizing the maximum co.pdf:application/pdf},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	number = {3},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {asynchronous dynamic programming, Q-learning, reinforcement learning, temporal differences},
	pages = {279--292},
	file = {Full Text PDF:/home/daveg/Zotero/storage/VF2BNQLA/Watkins and Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@misc{noauthor_box2d_nodate,
	title = {{Box2D}: {Overview}},
	url = {https://box2d.org/documentation/index.html},
	file = {Box2D\: Overview:/home/daveg/Zotero/storage/FA5V42CV/index.html:text/html},
}

@article{masanti_alternative_2022,
	title = {Alternative {Models} for {Direct} {Policy} {Search} in {Reinforcement} {Learning} {Control} {Problems}},
	author = {Masanti, Corina},
	year = {2022},
	file = {Masanti - Alternative Models for Direct Policy Search in Rei.pdf:/home/daveg/Zotero/storage/5W6MSAXC/Masanti - Alternative Models for Direct Policy Search in Rei.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {978-0-262-33737-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
	keywords = {Computers / Artificial Intelligence / General, Computers / Computer Science},
}

@book{sutton_reinforcement_1998,
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-30384-2},
	shorttitle = {Reinforcement {Learning}},
	abstract = {Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = feb,
	year = {1998},
	keywords = {Computers / Artificial Intelligence / General},
}

@phdthesis{cuccu_extending_2018,
	address = {Fribourg, Switzerland},
	type = {{PhD} {Thesis}},
	title = {Extending the {Applicability} of {Neuroevolution}},
	school = {Université de Fribourg},
	author = {Cuccu, Giuseppe},
	year = {2018},
}

@misc{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	year = {2016},
	note = {\_eprint: 1606.01540},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI} {Gym} {Beta}},
	url = {https://openai.com/research/openai-gym-beta},
	abstract = {We’re releasing the public beta of OpenAI Gym, a toolkit for developing and comparing reinforcement learning (RL) algorithms. It consists of a growing suite of environments (from simulated robots to Atari games), and a site for comparing and reproducing results.},
	file = {Snapshot:/home/daveg/Zotero/storage/K8QYPZHA/openai-gym-beta.html:text/html},
}

@article{recht_tour_2019,
	title = {A {Tour} of {Reinforcement} {Learning}: {The} {View} from {Continuous} {Control}},
	volume = {2},
	doi = {10.1146/annurev-control-053018-023825},
	abstract = {This article surveys reinforcement learning from the perspective of optimization and control, with a focus on continuous control applications. It reviews the general formulation, terminology, and typical experimental implementations of reinforcement learning as well as competing solution paradigms. In order to compare the relative merits of various techniques, it presents a case study of the linear quadratic regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. It also describes how merging techniques from learning theory and control can provide nonasymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. The article concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.},
	number = {1},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Recht, Benjamin},
	year = {2019},
	pages = {253--279},
}

@article{michael_t_data_2014,
	title = {Data {Structures} and {Algorithms} in {Python}},
	author = {Michael T, Goodrich and Roberto, Tamassia and Michael H, Goldwasser},
	year = {2014},
	note = {Publisher: wiley},
}

@article{elsken_neural_2019,
	title = {Neural architecture search: {A} survey},
	volume = {20},
	issn = {1532-4435},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	year = {2019},
	note = {Publisher: JMLR. org},
	pages = {1997--2017},
}

@inproceedings{mellor_neural_2021,
	title = {Neural architecture search without training},
	isbn = {2640-3498},
	publisher = {PMLR},
	author = {Mellor, Joe and Turner, Jack and Storkey, Amos and Crowley, Elliot J},
	year = {2021},
	pages = {7588--7598},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement learning: {An} introduction},
	isbn = {0-262-35270-2},
	publisher = {MIT press},
	author = {Sutton, Richard S and Barto, Andrew G},
	year = {2018},
}

@book{franklin_feedback_2002,
	title = {Feedback control of dynamic systems},
	volume = {4},
	publisher = {Prentice hall Upper Saddle River},
	author = {Franklin, Gene F and Powell, J David and Emami-Naeini, Abbas and Powell, J David},
	year = {2002},
}

@article{coggan_exploration_2004,
	title = {Exploration and exploitation in reinforcement learning},
	journal = {Research supervised by Prof. Doina Precup, CRA-W DMP Project at McGill University},
	author = {Coggan, Melanie},
	year = {2004},
}

@article{zhu_ingredients_2020,
	title = {The ingredients of real-world robotic reinforcement learning},
	journal = {arXiv preprint arXiv:2004.12570},
	author = {Zhu, Henry and Yu, Justin and Gupta, Abhishek and Shah, Dhruv and Hartikainen, Kristian and Singh, Avi and Kumar, Vikash and Levine, Sergey},
	year = {2020},
}

@article{schaul_studies_2011,
	title = {Studies in {Continuous} {Black}-box {Optimization}},
	author = {Schaul, Tom},
	year = {2011},
	note = {Publisher: Technische Universität München},
}

@book{anderson_introduction_1995,
	title = {An introduction to neural networks},
	isbn = {0-262-51081-2},
	publisher = {MIT press},
	author = {Anderson, James A},
	year = {1995},
}

@article{schmidhuber_evaluating_2001,
	title = {Evaluating benchmark problems by random guessing},
	journal = {A Field Guide to Dynamical Recurrent Networks},
	author = {Schmidhuber, Jürgen and Hochreiter, Sepp and Bengio, Yoshua},
	year = {2001},
	note = {Publisher: Wiley-IEEE Press, Hoboken, NJ, USA},
	pages = {231--235},
}

@inproceedings{cuccu_playing_2019,
	title = {Playing {Atari} with six neurons},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Cuccu, Giuseppe and Togelius, Julian and Cudré-Mauroux, Philippe},
	year = {2019},
	pages = {998--1006},
}

@inproceedings{oller_analyzing_2020,
	title = {Analyzing {Reinforcement} {Learning} {Benchmarks} with {Random} {Weight} {Guessing}},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Oller, Declan and Glasmachers, Tobias and Cuccu, Giuseppe},
	year = {2020},
}

@article{salimans_evolution_2017,
	title = {Evolution strategies as a scalable alternative to reinforcement learning},
	journal = {arXiv preprint arXiv:1703.03864},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
	year = {2017},
}

@misc{hansen_cma-espycma_2019,
	title = {{CMA}-{ES}/pycma on {Github}},
	author = {Hansen, Nikolaus and Akimoto, Youhei and Baudis, Petr},
	month = feb,
	year = {2019},
	doi = {10.5281/zenodo.2559634},
	note = {Published: Zenodo, DOI:10.5281/zenodo.2559634},
}
