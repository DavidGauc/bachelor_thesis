
@article{zhu_ingredients_2020,
	title = {{THE} {INGREDIENTS} {OF} {REAL}-{WORLD} {ROBOTIC} {REINFORCEMENT} {LEARNING}},
	abstract = {The success of reinforcement learning for real world robotics has been, in many cases limited to instrumented laboratory scenarios, often requiring arduous human effort and oversight to enable continuous learning. In this work, we discuss the elements that are needed for a robotic learning system that can continually and autonomously improve with data collected in the real world. We propose a particular instantiation of such a system, using dexterous manipulation as our case study. Subsequently, we investigate a number of challenges that come up when learning without instrumentation. In such settings, learning must be feasible without manually designed resets, using only on-board perception, and without hand-engineered reward functions. We propose simple and scalable solutions to these challenges, and then demonstrate the efﬁcacy of our proposed system on a set of dexterous robotic manipulation tasks, providing an in-depth analysis of the challenges associated with this learning paradigm. We demonstrate that our complete system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-ﬁngered hand. Results and videos can be found at https://sites.google.com/view/realworld-rl/.},
	language = {en},
	author = {Zhu, Henry and Yu, Justin and Gupta, Abhishek and Shah, Dhruv and Hartikainen, Kristian and Singh, Avi and Kumar, Vikash and Levine, Sergey},
	year = {2020},
	pages = {21},
	file = {Zhu et al. - 2020 - THE INGREDIENTS OF REAL-WORLD ROBOTIC REINFORCEMEN.pdf:/home/daveg/Zotero/storage/8EP8NIDY/Zhu et al. - 2020 - THE INGREDIENTS OF REAL-WORLD ROBOTIC REINFORCEMEN.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}, second edition: {An} {Introduction}},
	isbn = {978-0-262-35270-3},
	shorttitle = {Reinforcement {Learning}, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = nov,
	year = {2018},
	note = {Google-Books-ID: uWV0DwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General},
}

@article{barron_bellman_1989,
	title = {The {Bellman} equation for minimizing the maximum cost},
	volume = {13},
	issn = {0362546X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0362546X89900965},
	doi = {10.1016/0362-546X(89)90096-5},
	language = {en},
	number = {9},
	urldate = {2022-12-16},
	journal = {Nonlinear Analysis: Theory, Methods \& Applications},
	author = {Barron, E.N. and Ishii, H.},
	month = sep,
	year = {1989},
	pages = {1067--1090},
	file = {Barron and Ishii - 1989 - The Bellman equation for minimizing the maximum co.pdf:/home/daveg/Zotero/storage/VYN8LUJG/Barron and Ishii - 1989 - The Bellman equation for minimizing the maximum co.pdf:application/pdf},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	language = {en},
	number = {3},
	urldate = {2022-12-16},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {asynchronous dynamic programming, Q-learning, reinforcement learning, temporal differences},
	pages = {279--292},
	file = {Full Text PDF:/home/daveg/Zotero/storage/VF2BNQLA/Watkins and Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@book{anderson_introduction_1995,
	title = {An {Introduction} to {Neural} {Networks}},
	isbn = {978-0-262-51081-3},
	abstract = {An Introduction to Neural Networks falls into a new ecological niche for texts. Based on notes that have been class-tested for more than a decade, it is aimed at cognitive science and neuroscience students who need to understand brain function in terms of computational modeling, and at engineers who want to go beyond formal algorithms to applications and computing strategies. It is the only current text to approach networks from a broad neuroscience and cognitive science perspective, with an emphasis on the biology and psychology behind the assumptions of the models, as well as on what the models might be used for. It describes the mathematical and computational tools needed and provides an account of the author's own ideas. Students learn how to teach arithmetic to a neural network and get a short course on linear associative memory and adaptive maps. They are introduced to the author's brain-state-in-a-box (BSB) model and are provided with some of the neurobiological background necessary for a firm grasp of the general subject. The field now known as neural networks has split in recent years into two major groups, mirrored in the texts that are currently available: the engineers who are primarily interested in practical applications of the new adaptive, parallel computing technology, and the cognitive scientists and neuroscientists who are interested in scientific applications. As the gap between these two groups widens, Anderson notes that the academics have tended to drift off into irrelevant, often excessively abstract research while the engineers have lost contact with the source of ideas in the field. Neuroscience, he points out, provides a rich and valuable source of ideas about data representation and setting up the data representation is the major part of neural network programming. Both cognitive science and neuroscience give insights into how this can be done effectively: cognitive science suggests what to compute and neuroscience suggests how to compute it.},
	language = {en},
	publisher = {MIT Press},
	author = {Anderson, James A.},
	year = {1995},
	note = {Google-Books-ID: \_ib4vPdB76gC},
	keywords = {Computers / Artificial Intelligence / General, Social Science / Anthropology / Cultural \& Social, Technology \& Engineering / History},
}

@article{schaul_studies_nodate,
	title = {Studies in {Continuous} {Black}-box {Optimization}},
	language = {en},
	author = {Schaul, Tom},
	file = {Schaul - Studies in Continuous Black-box Optimization.pdf:/home/daveg/Zotero/storage/3VFHUB8U/Schaul - Studies in Continuous Black-box Optimization.pdf:application/pdf},
}

@misc{salimans_evolution_2017,
	title = {Evolution {Strategies} as a {Scalable} {Alternative} to {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.03864},
	abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Qlearning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
	language = {en},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
	month = sep,
	year = {2017},
	note = {arXiv:1703.03864 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf:/home/daveg/Zotero/storage/4UNTVLWM/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf:application/pdf},
}

@misc{noauthor_openai_2016,
	title = {{OpenAI} {Gym} {Beta}},
	url = {https://openai.com/blog/openai-gym-beta/},
	abstract = {We're releasing the public beta of OpenAI Gym [https://gym.openai.com/], a
toolkit for developing and comparing reinforcement learning (RL) algorithms. It
consists of a growing suite of environments [https://gym.openai.com/envs] (from 
simulated robots [https://gym.openai.com/envs/Humanoid-v0] to Atari
[https://gym.openai.com/},
	language = {en},
	urldate = {2023-01-08},
	journal = {OpenAI},
	month = apr,
	year = {2016},
	file = {Snapshot:/home/daveg/Zotero/storage/A749HQ8R/openai-gym-beta.html:text/html},
}

@misc{noauthor_box2d_nodate,
	title = {{Box2D}: {Overview}},
	url = {https://box2d.org/documentation/index.html},
	urldate = {2023-01-20},
	file = {Box2D\: Overview:/home/daveg/Zotero/storage/FA5V42CV/index.html:text/html},
}

@article{elsken_neural_nodate,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	language = {en},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	file = {Elsken et al. - Neural Architecture Search A Survey.pdf:/home/daveg/Zotero/storage/Y96PIE8L/Elsken et al. - Neural Architecture Search A Survey.pdf:application/pdf},
}

@misc{oller_analyzing_2020,
	title = {Analyzing {Reinforcement} {Learning} {Benchmarks} with {Random} {Weight} {Guessing}},
	url = {http://arxiv.org/abs/2004.07707},
	abstract = {We propose a novel method for analyzing and visualizing the complexity of standard reinforcement learning (RL) benchmarks based on score distributions. A large number of policy networks are generated by randomly guessing their parameters, and then evaluated on the benchmark task; the study of their aggregated results provide insights into the benchmark complexity. Our method guarantees objectivity of evaluation by sidestepping learning altogether: the policy network parameters are generated using Random Weight Guessing (RWG), making our method agnostic to (i) the classic RL setup, (ii) any learning algorithm, and (iii) hyperparameter tuning. We show that this approach isolates the environment complexity, highlights speciﬁc types of challenges, and provides a proper foundation for the statistical analysis of the task’s difﬁculty. We test our approach on a variety of classic control benchmarks from the OpenAI Gym, where we show that small untrained networks can provide a robust baseline for a variety of tasks. The networks generated often show good performance even without gradual learning, incidentally highlighting the triviality of a few popular benchmarks.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Oller, Declan and Glasmachers, Tobias and Cuccu, Giuseppe},
	month = apr,
	year = {2020},
	note = {arXiv:2004.07707 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Multiagent Systems},
	file = {Oller et al. - 2020 - Analyzing Reinforcement Learning Benchmarks with R.pdf:/home/daveg/Zotero/storage/TNN85XTV/Oller et al. - 2020 - Analyzing Reinforcement Learning Benchmarks with R.pdf:application/pdf},
}

@article{akimoto_theoretical_2012,
	title = {Theoretical {Foundation} for {CMA}-{ES} from {Information} {Geometry} {Perspective}},
	volume = {64},
	issn = {0178-4617, 1432-0541},
	url = {http://link.springer.com/10.1007/s00453-011-9564-8},
	doi = {10.1007/s00453-011-9564-8},
	abstract = {This paper explores the theoretical basis of the covariance matrix adaptation evolution strategy (CMA-ES) from the information geometry viewpoint. To establish a theoretical foundation for the CMA-ES, we focus on a geometric structure of a Riemannian manifold of probability distributions equipped with the Fisher metric. We deﬁne a function on the manifold which is the expectation of ﬁtness over the sampling distribution, and regard the goal of update of the parameters of sampling distribution in the CMA-ES as maximization of the expected ﬁtness. We investigate the steepest ascent learning for the expected ﬁtness maximization, where the steepest ascent direction is given by the natural gradient, which is the product of the inverse of the Fisher information matrix and the conventional gradient of the function.},
	language = {en},
	number = {4},
	urldate = {2023-01-24},
	journal = {Algorithmica},
	author = {Akimoto, Youhei and Nagata, Yuichi and Ono, Isao and Kobayashi, Shigenobu},
	month = dec,
	year = {2012},
	pages = {698--716},
	file = {Akimoto et al. - 2012 - Theoretical Foundation for CMA-ES from Information.pdf:/home/daveg/Zotero/storage/L8HHKE2I/Akimoto et al. - 2012 - Theoretical Foundation for CMA-ES from Information.pdf:application/pdf},
}

@article{masanti_alternative_nodate,
	title = {Alternative {Models} for {Direct} {Policy} {Search} in {Reinforcement} {Learning} {Control} {Problems}},
	language = {en},
	author = {Masanti, Corina},
	file = {Masanti - Alternative Models for Direct Policy Search in Rei.pdf:/home/daveg/Zotero/storage/5W6MSAXC/Masanti - Alternative Models for Direct Policy Search in Rei.pdf:application/pdf},
}

@article{mellor_neural_nodate,
	title = {Neural {Architecture} {Search} without {Training}},
	abstract = {The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be alleviated if we could partially predict a network’s trained accuracy from its initial state. In this work, we examine the overlap of activations between datapoints in untrained networks and motivate how this can give a measure which is usefully indicative of a network’s trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU, and verify its effectiveness on NAS-Bench-101, NASBench-201, NATS-Bench, and Network Design Spaces. Our approach can be readily combined with more expensive search methods; we examine a simple adaptation of regularised evolutionary search. Code for reproducing our experiments is available at https://github.com/ BayesWatch/nas-without-training.},
	language = {en},
	author = {Mellor, Joseph and Turner, Jack and Storkey, Amos and Crowley, Elliot J},
	file = {Mellor et al. - Neural Architecture Search without Training.pdf:/home/daveg/Zotero/storage/BTHMWL49/Mellor et al. - Neural Architecture Search without Training.pdf:application/pdf},
}

@article{coggan_exploration_nodate,
	title = {Exploration and {Exploitation} in {Reinforcement} {Learning}},
	language = {en},
	author = {Coggan, Melanie},
	file = {Coggan - Exploration and Exploitation in Reinforcement Lear.pdf:/home/daveg/Zotero/storage/TVKWHEMY/Coggan - Exploration and Exploitation in Reinforcement Lear.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {978-0-262-33737-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	language = {en},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
	note = {Google-Books-ID: omivDQAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Computer Science},
}

@book{sutton_reinforcement_1998,
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-30384-2},
	shorttitle = {Reinforcement {Learning}},
	abstract = {Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = feb,
	year = {1998},
	note = {Google-Books-ID: U57uDwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General},
}

@misc{recht_tour_2018,
	title = {A {Tour} of {Reinforcement} {Learning}: {The} {View} from {Continuous} {Control}},
	shorttitle = {A {Tour} of {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.09460},
	abstract = {This manuscript surveys reinforcement learning from the perspective of optimization and control with a focus on continuous control applications. It surveys the general formulation, terminology, and typical experimental implementations of reinforcement learning and reviews competing solution paradigms.},
	language = {en},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Recht, Benjamin},
	month = nov,
	year = {2018},
	note = {arXiv:1806.09460 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {Recht - 2018 - A Tour of Reinforcement Learning The View from Co.pdf:/home/daveg/Zotero/storage/ZM62BC9W/Recht - 2018 - A Tour of Reinforcement Learning The View from Co.pdf:application/pdf},
}

@misc{cuccu_playing_2019,
	title = {Playing {Atari} with {Six} {Neurons}},
	url = {http://arxiv.org/abs/1806.01363},
	abstract = {Deep reinforcement learning, applied to vision-based problems like Atari games, maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. By separating the image processing from decision-making, one could better understand the complexity of each task, as well as potentially ﬁnd smaller policy representations that are easier for humans to understand and may generalize better. To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations as they appear in an open-ended online-learning context; Direct Residuals Sparse Coding encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion. The encoder autonomously selects observations online to train on, in order to maximize code sparsity. As the dictionary size increases, the encoder produces increasingly larger inputs for the neural network: this is addressed by a variation of the Exponential Natural Evolution Strategies algorithm which adapts its probability distribution dimensionality along the run. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game’s controls). These are still capable of achieving results comparable—and occasionally superior—to state-of-the-art techniques which use two orders of magnitude more neurons.},
	language = {en},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Cuccu, Giuseppe and Togelius, Julian and Cudre-Mauroux, Philippe},
	month = mar,
	year = {2019},
	note = {arXiv:1806.01363 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Cuccu et al. - 2019 - Playing Atari with Six Neurons.pdf:/home/daveg/Zotero/storage/BLEESBP2/Cuccu et al. - 2019 - Playing Atari with Six Neurons.pdf:application/pdf},
}

@book{franklin_feedback_2014,
	address = {USA},
	edition = {7th},
	title = {Feedback {Control} of {Dynamic} {Systems}},
	isbn = {0-13-349659-7},
	abstract = {Feedback Control of Dynamic Systems covers the material that every engineer, and most scientists and prospective managers, needs to know about feedback controlincluding concepts like stability, tracking, and robustness. Each chapter presents the fundamentals along with comprehensive, worked-out examples, all within a real-world context and with historical background information. The authors also provide case studies with close integration of MATLAB throughout. Teaching and Learning Experience This program will provide a better teaching and learning experiencefor you and your students. It will provide: An Understandable Introduction to Digital Control: This text is devoted to supporting students equally in their need to grasp both traditional and more modern topics of digital control. Real-world Perspective: Comprehensive Case Studies and extensive integrated MATLAB/SIMULINK examples illustrate real-world problems and applications. Focus on Design: The authors focus on design as a theme early on and throughout the entire book, rather than focusing on analysis first and design much later.},
	publisher = {Prentice Hall Press},
	author = {Franklin, Gene F. and Powell, J. Da and Emami-Naeini, Abbas},
	year = {2014},
}

@misc{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym1 is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	language = {en},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv:1606.01540 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Brockman et al. - 2016 - OpenAI Gym.pdf:/home/daveg/Zotero/storage/CIQQX7E2/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf},
}

@article{goodrich_data_nodate,
	title = {Data {Structures} and {Algorithms} in {Python}},
	language = {en},
	author = {Goodrich, Michael T},
	file = {Goodrich - Data Structures and Algorithms in Python.pdf:/home/daveg/Zotero/storage/8Z6PD2VI/Goodrich - Data Structures and Algorithms in Python.pdf:application/pdf},
}
