
@article{zhu_ingredients_2020,
	title = {{THE} {INGREDIENTS} {OF} {REAL}-{WORLD} {ROBOTIC} {REINFORCEMENT} {LEARNING}},
	abstract = {The success of reinforcement learning for real world robotics has been, in many cases limited to instrumented laboratory scenarios, often requiring arduous human effort and oversight to enable continuous learning. In this work, we discuss the elements that are needed for a robotic learning system that can continually and autonomously improve with data collected in the real world. We propose a particular instantiation of such a system, using dexterous manipulation as our case study. Subsequently, we investigate a number of challenges that come up when learning without instrumentation. In such settings, learning must be feasible without manually designed resets, using only on-board perception, and without hand-engineered reward functions. We propose simple and scalable solutions to these challenges, and then demonstrate the efﬁcacy of our proposed system on a set of dexterous robotic manipulation tasks, providing an in-depth analysis of the challenges associated with this learning paradigm. We demonstrate that our complete system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-ﬁngered hand. Results and videos can be found at https://sites.google.com/view/realworld-rl/.},
	language = {en},
	author = {Zhu, Henry and Yu, Justin and Gupta, Abhishek and Shah, Dhruv and Hartikainen, Kristian and Singh, Avi and Kumar, Vikash and Levine, Sergey},
	year = {2020},
	pages = {21},
	file = {Zhu et al. - 2020 - THE INGREDIENTS OF REAL-WORLD ROBOTIC REINFORCEMEN.pdf:/home/daveg/Zotero/storage/8EP8NIDY/Zhu et al. - 2020 - THE INGREDIENTS OF REAL-WORLD ROBOTIC REINFORCEMEN.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}, second edition: {An} {Introduction}},
	isbn = {978-0-262-35270-3},
	shorttitle = {Reinforcement {Learning}, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = nov,
	year = {2018},
	note = {Google-Books-ID: uWV0DwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General},
}

@article{barron_bellman_1989,
	title = {The {Bellman} equation for minimizing the maximum cost},
	volume = {13},
	issn = {0362546X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0362546X89900965},
	doi = {10.1016/0362-546X(89)90096-5},
	language = {en},
	number = {9},
	urldate = {2022-12-16},
	journal = {Nonlinear Analysis: Theory, Methods \& Applications},
	author = {Barron, E.N. and Ishii, H.},
	month = sep,
	year = {1989},
	pages = {1067--1090},
	file = {Barron and Ishii - 1989 - The Bellman equation for minimizing the maximum co.pdf:/home/daveg/Zotero/storage/VYN8LUJG/Barron and Ishii - 1989 - The Bellman equation for minimizing the maximum co.pdf:application/pdf},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	language = {en},
	number = {3},
	urldate = {2022-12-16},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {asynchronous dynamic programming, Q-learning, reinforcement learning, temporal differences},
	pages = {279--292},
	file = {Full Text PDF:/home/daveg/Zotero/storage/VF2BNQLA/Watkins and Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@book{anderson_introduction_1995,
	title = {An {Introduction} to {Neural} {Networks}},
	isbn = {978-0-262-51081-3},
	abstract = {An Introduction to Neural Networks falls into a new ecological niche for texts. Based on notes that have been class-tested for more than a decade, it is aimed at cognitive science and neuroscience students who need to understand brain function in terms of computational modeling, and at engineers who want to go beyond formal algorithms to applications and computing strategies. It is the only current text to approach networks from a broad neuroscience and cognitive science perspective, with an emphasis on the biology and psychology behind the assumptions of the models, as well as on what the models might be used for. It describes the mathematical and computational tools needed and provides an account of the author's own ideas. Students learn how to teach arithmetic to a neural network and get a short course on linear associative memory and adaptive maps. They are introduced to the author's brain-state-in-a-box (BSB) model and are provided with some of the neurobiological background necessary for a firm grasp of the general subject. The field now known as neural networks has split in recent years into two major groups, mirrored in the texts that are currently available: the engineers who are primarily interested in practical applications of the new adaptive, parallel computing technology, and the cognitive scientists and neuroscientists who are interested in scientific applications. As the gap between these two groups widens, Anderson notes that the academics have tended to drift off into irrelevant, often excessively abstract research while the engineers have lost contact with the source of ideas in the field. Neuroscience, he points out, provides a rich and valuable source of ideas about data representation and setting up the data representation is the major part of neural network programming. Both cognitive science and neuroscience give insights into how this can be done effectively: cognitive science suggests what to compute and neuroscience suggests how to compute it.},
	language = {en},
	publisher = {MIT Press},
	author = {Anderson, James A.},
	year = {1995},
	note = {Google-Books-ID: \_ib4vPdB76gC},
	keywords = {Computers / Artificial Intelligence / General, Social Science / Anthropology / Cultural \& Social, Technology \& Engineering / History},
}

@article{schaul_studies_nodate,
	title = {Studies in {Continuous} {Black}-box {Optimization}},
	language = {en},
	author = {Schaul, Tom},
	file = {Schaul - Studies in Continuous Black-box Optimization.pdf:/home/daveg/Zotero/storage/3VFHUB8U/Schaul - Studies in Continuous Black-box Optimization.pdf:application/pdf},
}

@misc{salimans_evolution_2017,
	title = {Evolution {Strategies} as a {Scalable} {Alternative} to {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.03864},
	abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Qlearning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
	language = {en},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
	month = sep,
	year = {2017},
	note = {arXiv:1703.03864 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf:/home/daveg/Zotero/storage/4UNTVLWM/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf:application/pdf},
}

@misc{noauthor_openai_2016,
	title = {{OpenAI} {Gym} {Beta}},
	url = {https://openai.com/blog/openai-gym-beta/},
	abstract = {We're releasing the public beta of OpenAI Gym [https://gym.openai.com/], a
toolkit for developing and comparing reinforcement learning (RL) algorithms. It
consists of a growing suite of environments [https://gym.openai.com/envs] (from 
simulated robots [https://gym.openai.com/envs/Humanoid-v0] to Atari
[https://gym.openai.com/},
	language = {en},
	urldate = {2023-01-08},
	journal = {OpenAI},
	month = apr,
	year = {2016},
	file = {Snapshot:/home/daveg/Zotero/storage/A749HQ8R/openai-gym-beta.html:text/html},
}

@misc{noauthor_box2d_nodate,
	title = {{Box2D}: {Overview}},
	url = {https://box2d.org/documentation/index.html},
	urldate = {2023-01-20},
	file = {Box2D\: Overview:/home/daveg/Zotero/storage/FA5V42CV/index.html:text/html},
}

@article{elsken_neural_nodate,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	language = {en},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	file = {Elsken et al. - Neural Architecture Search A Survey.pdf:/home/daveg/Zotero/storage/Y96PIE8L/Elsken et al. - Neural Architecture Search A Survey.pdf:application/pdf},
}
